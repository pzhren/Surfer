<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A World Model-Based Framework for Vision-Language Robot Manipulation">
  <meta name="keywords" content="Robotics Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Surfer: A World Model-Based Framework for Vision-Language Robot Manipulation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="highlight">Surfer</span>: Progressive Reasoning with World Models for Robotic Manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Pengzhen Ren<sup>1</sup>,
              Kaidong Zhang<sup>1</sup>,
              Hetao Zheng<sup>1</sup>,
              Zixuan Li<sup>1</sup>, 
              Yuhang Wen<sup>1</sup>,
            </span>
            <span class="author-block">
              Fengda Zhu<sup>2</sup>,
              Mas Ma<sup>3</sup>,
              Xiaodan Liang<sup>1,4*</sup>
            </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>Sun Yat-sen University&nbsp;</span>
            <span class="author-block"><sup>2</sup>Monash University&nbsp;</span>
            <span class="author-block"><sup>3</sup>Dataa Robotics&nbsp;</span>
            <span class="author-block"><sup>4</sup>MBZUAI</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/Necolizer/RM-PRT"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2306.11335"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/Necolizer/RM-PRT"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Necolizer/RM-PRT"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Simulator Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1jLXAU9eHE6rcpLtohepGlb654mUbA4KR?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-download"></i>
                  </span>
                  <span>Simulator</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Considering how to make the model accurately understand and follow natural language instructions and perform actions consistent with world knowledge is a key challenge in robot manipulation. This mainly includes human fuzzy instruction reasoning and the following of physical knowledge. Therefore, the embodied intelligence agent must have the ability to model world knowledge from training data. However, most existing vision and language robot manipulation methods mainly operate in less realistic simulators and language settings and lack explicit modeling of world knowledge.
          </p>
          <p>
          To bridge this gap, we introduce a novel and simple robot manipulation framework, called <strong>Surfer</strong>. It is based on the world model, treats robot manipulation as a state transfer of the visual scene, and decouples it into two parts: action and scene. Then, the generalization ability of the model on new instructions and new scenes is enhanced by explicit modeling of the action and scene prediction in multi-modal information. In addition to the framework, we also built a robot manipulation simulator that supports physics execution based on the MuJoCo physics engine. It can automatically generate demonstration training data and test data, effectively reducing labor costs.
          </p>
          <p>
          To conduct a comprehensive and systematic evaluation of the robot manipulation model in terms of language understanding and physical execution, we also created a robotic manipulation benchmark with progressive reasoning tasks, called <strong>SeaWave</strong>. It contains 4 levels of progressive reasoning tasks and can provide a standardized testing platform for embedded AI agents in multi-modal environments. <em>Overall, we hope Surfer can freely surf in the robot's SeaWave benchmark.</em> Extensive experiments show that Surfer consistently outperforms all baselines by a significant margin in all manipulation tasks. On average, Surfer achieved a success rate of 54.74% on the defined four levels of manipulation tasks, exceeding the best baseline performance of 51.07%.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-five-fifths">
      <!-- <div class="hero-body"> -->
        
        <h2 class="title is-3">Surfer</h2>
        <div class="content has-text-justified">
        <p>
          The overall framework of Surfer. It is a manipulation model based on the world model to simulate the physical laws of robot action execution. Sufer’s world model is mainly composed of an action prediction module and a scene prediction module. They are guided by language instructions P and predicted actions A′ respectively, helping the model learn physical laws from the robot’s historical action trajectories I.
        </p>
      </div>
          <img src="./static/images/model.png" alt>
        <!-- <h2 class="subtitle has-text-centered">
          The overall framework of Surfer.
        </h2> -->
      <!-- </div> -->
    </div>
    </div>
    </div>


    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Real Machine Demo</h2>
        <div class="publication-video">
          <video id="3" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/3.mp4"
                  type="video/mp4">
          </video>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">SeaWave</h2>
        <!-- <div class="publication-video">
          <video id="1" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/1.mp4"
                  type="video/mp4">
          </video>
        </div> -->

        <!-- <div class="publication-video">
          <video id="2" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/2.mp4"
                  type="video/mp4">
          </video>
        </div> -->
    
        <!-- <h2 class="title is-3">Workspace</h2> -->
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/k6V9LHUi2hw?rel=0&amp;showinfo=0" title="YouTube video player" frameborder="0" allow="autoplay; encrypted-media; gyroscope" allowfullscreen></iframe>
        </div>
      </div>
      </div>
    </div>
</section>




<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-1">
          <img src="./static/images/render/1.jpg" alt>
        </div>
        <div class="item item-2">
          <img src="./static/images/render/2.jpg" alt>
        </div>
        <div class="item item-3">
          <img src="./static/images/render/3.jpg" alt>
        </div>
        <div class="item item-4">
          <img src="./static/images/render/4.jpg" alt>
        </div>
      </div>
    </div>
  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-five-fifths">
        <!-- Pipeline. -->
        <h2 class="title is-3">Pipeline</h2>
        <!-- <div class="content has-text-justified">
          
        </div> -->
        <img src="./static/images/pipeline.png" alt>
        
        <!--/ Pipeline. -->
        <br><br>
        <!-- Tasks. -->
        <h2 class="title is-3">Progressive Reasoning Tasks</h2>
        <img src="./static/images/multi_level_tasks.jpg" alt>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Experiment Results</h2>
      </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-five-fifths">
        <!-- Pipeline. -->
        <h2 class="title is-3"> Results on Robotic Manipulation</h2>
        <p>In this section, we evaluate the performance of Surfer in progressive instruction reasoning tasks. In the SeaWave benchmark, the training and testing environments mainly include instructions and scenes <em>(i.e.)</em>, objects, tables, spatial positional relationships, and rotation information, etc. Because the scenarios are regenerated each time, the scene is unseen for each level of task. At the time of testing, since the level 1 instructions are in the form of "verb + noun", they are seen. For levels 2, 3, and 4, human-like natural language instructions are used, so they are unseen, and the corresponding number of instructions is 80, 172, and 226, respectively.</p>
        <p>Compared to other baselines, Surfer achieves the best manipulation success rate on almost all tasks. Specifically, compared to baseline RT-1, Surfer's manipulation success rate increased by an average of 7.1% on four levels of progressive reasoning tasks. It is worth noting that Surfer still has a 3.15% (37.89% <em>vs.</em> 34.74%) improvement in the most challenging level 4 tasks. In addition, compared with GR-1, which also uses scene prediction ideas, Surfer still achieved a significant improvement of 3.67% (54.74% <em>vs</em> 51.07%) on average on four levels of manipulation tasks. This shows that Surfer is very effective in modeling robot manipulators' perception of scene changes by world model associated action on prediction and scene prediction.</p>

        <img src="./static/images/table1.png" alt>
        
        <!--/ Pipeline. -->
        <br><br>
        <!-- Tasks. -->
        <h2 class="title is-3">Model Robustness</h2>
        <p>Here, we evaluate the robustness of Surfer in different scenarios using the average manipulation success rate of the model on level 2, 3, and 4 tasks. New scenarios include unseen backgrounds <em>(i.e.), unseen tables</em>, changing light intensity, and more distractions <em>(i.e.), more objects. These scenarios were never seen during model training.</p>
        <p>We found that more distractors significantly reduced the success rate of model manipulation. In particular, we find that Octo maintains the best robustness compared to other methods, which may be because it uses more training trajectories.</p>
        <p>Overall, under various interference factors, Surfer still maintains a significant performance advantage compared to other baselines (e.g., Surfer vs RT-1: 44.44% vs 38.05%). And under three different scene changes, the average performance loss ratio of Surfer is only 8%. In comparison, the performance loss ratios of GR-1 and BC-Z reached 11% and 43%, respectively. This indicates that Surfer has good robustness with the help of the world model.</p>
        <img src="./static/images/table2.png" alt>
        <br><br>

        <h2 class="title is-3">Videos</h2>
        <div class="publication-video" style="text-align: center; font-size: 24px; font-weight: bold;">
          <video id="3" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/examples1.mp4"
                  type="video/mp4">
          </video>
          <p>Please <span style="color: red;">pick up</span> the item that is positioned <span style="color: blue;">behind right</span>.</p>
        </div>
        <div class="publication-video" style="text-align: center; font-size: 24px; font-weight: bold;">
          <video id="3" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/examples2.mp4"
                  type="video/mp4">
          </video>
          <p>Can you <span style="color: red;">put</span> the <span style="color: blue;">green bottle</span> and the <span style="color: blue;">chips</span> <span style="color: red;">together?</span></p>
        </div>

        <br><br>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{ren2024surferprogressivereasoningworld,
      title={Surfer: Progressive Reasoning with World Models for Robotic Manipulation}, 
      author={Pengzhen Ren and Kaidong Zhang and Hetao Zheng and Zixuan Li and Yuhang Wen and Fengda Zhu and Mas Ma and Xiaodan Liang},
      year={2024},
      eprint={2306.11335},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2306.11335}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="https://github.com/Necolizer/RM-PRT">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <a class="icon-link" href="https://github.com/Necolizer/RM-PRT" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
        <div class="content">
          <p>
            Template of this page is borrowed from <a
              href="https://nerfies.github.io/">Nerfies</a>. Grateful to this great work.

          </p>
        </div>
    </div>
  </div>
</footer>

</body>
</html>
